#!/usr/bin/env node

// Simple test to see if LLM is responding
import { localLLM } from './src/ai/localLLM.js';

async function testLLM() {
  console.log('üß™ Testing LLM connection...\n');

  try {
    const response = await localLLM({
      model: 'phi3:mini',
      prompt: 'Hallo, bist du da? Antworte mit JA oder NEIN.',
      stream: false
    });

    console.log('‚úÖ LLM Response:');
    console.log(response.response);
    console.log('\n‚úÖ LLM is working!');

  } catch (error) {
    console.error('‚ùå LLM Error:', error.message);
    console.error('\n‚ÑπÔ∏è  Make sure Ollama is running: ollama serve');
    console.error('‚ÑπÔ∏è  And phi3:mini model is pulled: ollama pull phi3:mini');
  }
}

testLLM();
